diff -urN original/fs/f2fs/checkpoint.c linux/fs/f2fs/checkpoint.c
--- original/fs/f2fs/checkpoint.c	2020-08-18 21:43:20.171705348 +0900
+++ linux/fs/f2fs/checkpoint.c	2020-12-04 15:14:53.051773756 +0900
@@ -1110,6 +1110,7 @@
 	up_write(&sbi->node_change);
 out:
 	blk_finish_plug(&plug);
+
 	return err;
 }
 
@@ -1393,6 +1394,8 @@
 	unsigned long long ckpt_ver;
 	int err = 0;
 
+	printk("[SION DBG] %s: begin", __func__);
+
 	mutex_lock(&sbi->cp_mutex);
 
 	if (!is_sbi_flag_set(sbi, SBI_IS_DIRTY) &&
@@ -1466,6 +1469,9 @@
 	trace_f2fs_write_checkpoint(sbi->sb, cpc->reason, "finish checkpoint");
 out:
 	mutex_unlock(&sbi->cp_mutex);
+
+	printk("[SION DBG] %s: end", __func__);
+
 	return err;
 }
 
diff -urN original/fs/f2fs/data.c linux/fs/f2fs/data.c
--- original/fs/f2fs/data.c	2020-08-18 21:43:20.171705348 +0900
+++ linux/fs/f2fs/data.c	2020-12-01 23:36:14.717808704 +0900
@@ -32,6 +32,7 @@
 
 static struct kmem_cache *bio_post_read_ctx_cache;
 static mempool_t *bio_post_read_ctx_pool;
+extern struct kmem_cache *inmem_entry_slab;
 
 static bool __is_cp_guaranteed(struct page *page)
 {
@@ -606,10 +607,45 @@
  */
 void f2fs_set_data_blkaddr(struct dnode_of_data *dn)
 {
+	struct inode *inode = dn->inode;
+
 	f2fs_wait_on_page_writeback(dn->node_page, NODE, true);
 	__set_data_blkaddr(dn);
 	if (set_page_dirty(dn->node_page))
 		dn->node_changed = true;
+
+	/*
+	 * If an address of the data block is allocated so direct node block
+	 * is modified during commit of multi-file atomic write, the modified
+	 * direct node block is added to inmem_node_pages in atomic file set.
+	 * It is for walking only modified node pages during commit.
+	 * - Joontaek Oh.
+	 */
+	if (f2fs_is_commit_atomic_write(inode) && f2fs_is_added_file(inode)) {
+		struct f2fs_inode_info *fi = F2FS_I(inode);
+		struct atomic_file_set *afs = fi->af->afs;
+		struct inmem_node_pages *inmem_node_cur, *inmem_node_tmp;
+		bool alloc = true;
+
+		list_for_each_entry_safe(inmem_node_cur, inmem_node_tmp, &afs->inmem_node_pages, list) {
+			if (dn->nid == inmem_node_cur->nid) {
+				alloc = false;
+				break;
+			}
+		}
+
+		if (alloc) {
+			struct inmem_node_pages *new;
+
+			new = f2fs_kmem_cache_alloc(inmem_entry_slab, GFP_NOFS);
+
+			new->page = dn->node_page;
+			INIT_LIST_HEAD(&new->list);
+			new->nid = dn->nid;
+
+			list_add_tail(&new->list, &fi->af->afs->inmem_node_pages);
+		}
+	}
 }
 
 void f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)
@@ -2074,6 +2110,13 @@
 	return ret;
 }
 
+int __f2fs_write_cache_pages(struct address_space *mapping,
+					struct writeback_control *wbc,
+					enum iostat_type io_type)
+{
+	return f2fs_write_cache_pages(mapping, wbc, io_type);
+}
+
 static int __f2fs_write_data_pages(struct address_space *mapping,
 						struct writeback_control *wbc,
 						enum iostat_type io_type)
@@ -2250,6 +2293,7 @@
 			!f2fs_available_free_memory(sbi, INMEM_PAGES)) {
 		err = -ENOMEM;
 		drop_atomic = true;
+		printk("[JATA DBG] (%s) no memory\n", __func__);
 		goto fail;
 	}
 
@@ -2342,6 +2386,14 @@
 {
 	struct inode *inode = page->mapping->host;
 
+/*
+	if (inode) {
+		struct dentry *dentry = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);
+		if (dentry)
+			printk("[JATA DBG] (%s) %u: %s\n", __func__, current->pid, dentry->d_name.name);
+	}
+*/
+
 	trace_f2fs_write_end(inode, pos, len, copied);
 
 	/*
@@ -2490,7 +2542,25 @@
 	if (!PageUptodate(page))
 		SetPageUptodate(page);
 
-	if (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {
+	if ((current->is_atomic || f2fs_is_atomic_file(inode)) && !f2fs_is_commit_atomic_write(inode)) {
+		if (!f2fs_is_atomic_file(inode) && current->is_atomic) {
+			//int locked = 0;
+			int ret;
+
+			/* We guaranteed that the caller already holds the inode lock.
+			 *
+			if (inode_is_locked(inode)) {
+				inode_unlock(inode);
+				locked = 1;
+			}*/
+
+			ret = f2fs_ioc_add_atomic_inode(inode, (unsigned long)&current->afs);
+
+			/*
+			if (locked)
+				inode_lock(inode);*/
+		}
+
 		if (!IS_ATOMIC_WRITTEN_PAGE(page)) {
 			f2fs_register_inmem_page(inode, page);
 			return 1;
diff -urN original/fs/f2fs/f2fs.h linux/fs/f2fs/f2fs.h
--- original/fs/f2fs/f2fs.h	2020-08-18 21:43:20.175705250 +0900
+++ linux/fs/f2fs/f2fs.h	2020-10-07 12:49:29.071677209 +0900
@@ -381,6 +381,14 @@
 #define F2FS_IOC_SET_PIN_FILE		_IOW(F2FS_IOCTL_MAGIC, 13, __u32)
 #define F2FS_IOC_GET_PIN_FILE		_IOR(F2FS_IOCTL_MAGIC, 14, __u32)
 #define F2FS_IOC_PRECACHE_EXTENTS	_IO(F2FS_IOCTL_MAGIC, 15)
+#define F2FS_IOC_ADD_ATOMIC_FILE	_IO(F2FS_IOCTL_MAGIC, 16)
+#define F2FS_IOC_START_ATOMIC_FILE_SET	_IO(F2FS_IOCTL_MAGIC, 17)
+#define F2FS_IOC_COMMIT_ATOMIC_FILE_SET	_IO(F2FS_IOCTL_MAGIC, 18)
+#define F2FS_IOC_END_ATOMIC_FILE_SET	_IO(F2FS_IOCTL_MAGIC, 19)
+
+#define F2FS_IOC_COMMIT_NOFLUSH_ATOMIC_FILE_SET	_IO(F2FS_IOCTL_MAGIC, 20)
+#define F2FS_IOC_COMMIT_NODMA_ATOMIC_FILE_SET	_IO(F2FS_IOCTL_MAGIC, 21)
+#define F2FS_IOC_COMMIT_NOFLUSHDMA_ATOMIC_FILE_SET	_IO(F2FS_IOCTL_MAGIC, 22)
 
 #define F2FS_IOC_SET_ENCRYPTION_POLICY	FS_IOC_SET_ENCRYPTION_POLICY
 #define F2FS_IOC_GET_ENCRYPTION_POLICY	FS_IOC_GET_ENCRYPTION_POLICY
@@ -619,6 +627,40 @@
 
 #define DEF_DIR_LEVEL		0
 
+/*
+ * This is for Multi File Transactional Write (MUFIT).
+ * 
+ * - MUFIT_NODE: Offset of mufit node block. It is similar to XATTR_NODE_OFFSET.
+ * - struct atomic_file_set: It represent a atomic file set.
+ * - struct atomic_file: It is pair of list_head and file. The purpose of this structure is listing of files to atomic file set.
+ *
+ * - Joontaek Oh.
+ */
+#define MASTER_NODE_OFFSET	((((unsigned int)-2) << OFFSET_BIT_SHIFT) >> OFFSET_BIT_SHIFT)
+#define F2FS_MUFIT_MAGIC	0xF2F52011
+
+struct atomic_file_set {
+	unsigned long key;			/* key for the atomic file set */
+	struct task_struct *owner;		/* Pointer to thread which have made it */
+	struct list_head afs_list;		/* atomic file list */
+	struct list_head inmem_pages;		/* inmemory pages managed by f2fs */
+	struct list_head inmem_node_pages;	/* inmemory pages for node */
+	struct rw_semaphore afs_rwsem;		/* semaphore for afs */
+	unsigned int commit_file_count;		/* commit count in atomic file set */
+	struct atomic_file *last_file;		/* last file in atomic file set */
+	nid_t master_nid;			/* nid of mufit node */
+	struct rhash_head khtnode;		/* node for key hash table */
+	__le32 afs_magic;			/* magic number for atomic file set */
+};
+
+struct atomic_file {
+	struct list_head list;
+	struct inode *inode;
+	struct list_head revoke_list;
+	struct atomic_file_set *afs; 	/* pointer to atomic file set that including this file */
+	bool last_file;
+};
+
 enum {
 	GC_FAILURE_PIN,
 	GC_FAILURE_ATOMIC,
@@ -671,6 +713,8 @@
 	int i_inline_xattr_size;	/* inline xattr size */
 	struct timespec i_crtime;	/* inode creation time */
 	struct timespec i_disk_time[4];	/* inode disk times */
+
+	struct atomic_file *af;	/* pointer to atomic file responds to this file */
 };
 
 static inline void get_extent_info(struct extent_info *ext,
@@ -1272,6 +1316,13 @@
 	/* Reference to checksum algorithm driver via cryptoapi */
 	struct crypto_shash *s_chksum_driver;
 
+	/* 
+	 * exF2FS: Hash Table for atomic file sets
+	 * - Joontaek Oh
+	 */
+	struct rhashtable afs_ht;
+	struct rhashtable_params afs_kht_params;
+
 	/* Precomputed FS UUID checksum for seeding other checksums */
 	__u32 s_chksum_seed;
 };
@@ -2259,6 +2310,7 @@
 	FI_PROJ_INHERIT,	/* indicate file inherits projectid */
 	FI_PIN_FILE,		/* indicate file should not be gced */
 	FI_ATOMIC_REVOKE_REQUEST, /* request to drop atomic data */
+	FI_ADDED_ATOMIC_FILE,	/* indicate file is in atomic file set */
 };
 
 static inline void __mark_inode_dirty_flag(struct inode *inode,
@@ -2466,6 +2518,11 @@
 	return is_inode_flag_set(inode, FI_ATOMIC_FILE);
 }
 
+static inline bool f2fs_is_added_file(struct inode *inode)
+{
+	return is_inode_flag_set(inode, FI_ADDED_ATOMIC_FILE);
+}
+
 static inline bool f2fs_is_commit_atomic_write(struct inode *inode)
 {
 	return is_inode_flag_set(inode, FI_ATOMIC_COMMIT);
@@ -2692,6 +2749,7 @@
 long f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
 long f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
 int f2fs_pin_file_control(struct inode *inode, bool inc);
+extern int f2fs_ioc_add_atomic_inode(struct inode *inode, unsigned long arg);
 
 /*
  * inode.c
@@ -2827,6 +2885,9 @@
 void f2fs_destroy_node_manager(struct f2fs_sb_info *sbi);
 int __init f2fs_create_node_manager_caches(void);
 void f2fs_destroy_node_manager_caches(void);
+int f2fs_build_master_node(struct atomic_file_set *afs);
+int f2fs_truncate_master_node(struct atomic_file_set *afs);
+extern int ____write_node_page(struct page *page, bool atomic, bool *submitted, struct writeback_control *wbc, bool do_balance, enum iostat_type io_type);
 
 /*
  * segment.c
@@ -2837,6 +2898,7 @@
 void f2fs_drop_inmem_pages(struct inode *inode);
 void f2fs_drop_inmem_page(struct inode *inode, struct page *page);
 int f2fs_commit_inmem_pages(struct inode *inode);
+int f2fs_commit_inmem_pages_atomic_file_set(struct inode *inode, struct list_head *revoke_list);
 void f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need);
 void f2fs_balance_fs_bg(struct f2fs_sb_info *sbi);
 int f2fs_issue_flush(struct f2fs_sb_info *sbi, nid_t ino);
@@ -2891,6 +2953,7 @@
 int f2fs_rw_hint_to_seg_type(enum rw_hint hint);
 enum rw_hint f2fs_io_type_to_rw_hint(struct f2fs_sb_info *sbi,
 			enum page_type type, enum temp_type temp);
+int ____revoke_inmem_pages(struct inode *inode, struct list_head *head, bool drop, bool recover);
 
 /*
  * checkpoint.c
@@ -2974,6 +3037,7 @@
 #endif
 bool f2fs_overwrite_io(struct inode *inode, loff_t pos, size_t len);
 void f2fs_clear_radix_tree_dirty_tag(struct page *page);
+extern int __f2fs_write_cache_pages(struct address_space *mapping, struct writeback_control *wbc, enum iostat_type io_type);
 
 /*
  * gc.c
@@ -3373,4 +3437,12 @@
 			F2FS_I_SB(inode)->s_ndevs);
 }
 
+// in kernel
+static inline long long get_current_utime(void)
+{
+        struct timeval current_time;
+        do_gettimeofday(&current_time);
+        return (current_time.tv_sec*1000000 + current_time.tv_usec);
+}
+
 #endif
diff -urN original/fs/f2fs/file.c linux/fs/f2fs/file.c
--- original/fs/f2fs/file.c	2020-08-18 21:43:20.175705250 +0900
+++ linux/fs/f2fs/file.c	2020-12-04 15:42:14.922578065 +0900
@@ -24,6 +24,9 @@
 #include <linux/uuid.h>
 #include <linux/file.h>
 
+#include <linux/delay.h>
+#include <linux/ktime.h>
+
 #include "f2fs.h"
 #include "node.h"
 #include "segment.h"
@@ -33,6 +36,8 @@
 #include "trace.h"
 #include <trace/events/f2fs.h>
 
+extern struct kmem_cache *inmem_entry_slab;
+
 static vm_fault_t f2fs_filemap_fault(struct vm_fault *vmf)
 {
 	struct inode *inode = file_inode(vmf->vma->vm_file);
@@ -219,6 +224,10 @@
 
 	trace_f2fs_sync_file_enter(inode);
 
+	/* If file is atomic file, then it should not be synced before tx committing it. */
+	if (f2fs_is_atomic_file(inode))
+		return 0;
+
 	/* if fdatasync is triggered, let's do in-place-update */
 	if (datasync || get_dirty_pages(inode) <= SM_I(sbi)->min_fsync_blocks)
 		set_inode_flag(inode, FI_NEED_IPU);
@@ -286,7 +295,7 @@
 		goto out;
 	}
 
-	if (f2fs_need_inode_block_update(sbi, ino)) {
+	if (f2fs_need_inode_block_update(sbi, ino) && !f2fs_is_added_file(inode)) {
 		f2fs_mark_inode_dirty_sync(inode, true);
 		f2fs_write_inode(inode, NULL);
 		goto sync_nodes;
@@ -1552,6 +1561,9 @@
 	return ret;
 }
 
+static int f2fs_ioc_end_atomic_file_set(struct file *filp, unsigned long arg);
+static int f2fs_ioc_end_atomic_file_set_thread(struct file *filp, struct atomic_file_set *afs);
+
 static int f2fs_release_file(struct inode *inode, struct file *filp)
 {
 	/*
@@ -1562,6 +1574,13 @@
 			atomic_read(&inode->i_writecount) != 1)
 		return 0;
 
+	if (f2fs_is_added_file(inode)) {
+		struct f2fs_inode_info *fi = F2FS_I(inode);
+
+		if (fi->af->afs->owner == current) {
+			f2fs_ioc_end_atomic_file_set(filp, (unsigned long) fi->af->afs);
+		}
+	}
 	/* some remained atomic pages should discarded */
 	if (f2fs_is_atomic_file(inode))
 		f2fs_drop_inmem_pages(inode);
@@ -1721,7 +1740,51 @@
 	return ret;
 }
 
-static int f2fs_ioc_commit_atomic_write(struct file *filp)
+static int f2fs_ioc_start_atomic_write_inode(struct inode *inode)
+{
+	int ret;
+
+	if (!inode_owner_or_capable(inode))
+		return -EACCES;
+
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	inode_lock(inode);
+
+	down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
+	if (f2fs_is_atomic_file(inode))
+		goto out;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		goto out;
+
+	if (!get_dirty_pages(inode))
+		goto skip_flush;
+
+	f2fs_msg(F2FS_I_SB(inode)->sb, KERN_WARNING,
+		"Unexpected flush for atomic writes: ino=%lu, npages=%u",
+					inode->i_ino, get_dirty_pages(inode));
+	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+	if (ret)
+		goto out;
+skip_flush:
+	set_inode_flag(inode, FI_ATOMIC_FILE);
+	clear_inode_flag(inode, FI_ATOMIC_REVOKE_REQUEST);
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+
+	F2FS_I(inode)->inmem_task = current;
+	stat_inc_atomic_write(inode);
+	stat_update_max_atomic_write(inode);
+out:
+	up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	inode_unlock(inode);
+	return ret;
+}
+
+static int f2fs_ioc_commit_atomic_write(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 	int ret;
@@ -1742,7 +1805,9 @@
 		goto err_out;
 	}
 
-	if (f2fs_is_atomic_file(inode)) {
+	if (f2fs_is_atomic_file(inode) || f2fs_is_added_file(inode)) {
+		struct f2fs_sb_info *sbi = F2FS_I_SB(inode);;
+
 		ret = f2fs_commit_inmem_pages(inode);
 		if (ret)
 			goto err_out;
@@ -1753,6 +1818,9 @@
 			F2FS_I(inode)->i_gc_failures[GC_FAILURE_ATOMIC] = 0;
 			stat_dec_atomic_write(inode);
 		}
+
+		if (arg)
+			ret = f2fs_wait_on_node_pages_writeback(sbi, inode->i_ino);
 	} else {
 		ret = f2fs_do_sync_file(filp, 0, LLONG_MAX, 1, false);
 	}
@@ -1764,6 +1832,7 @@
 	up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 	inode_unlock(inode);
 	mnt_drop_write_file(filp);
+
 	return ret;
 }
 
@@ -2851,6 +2920,961 @@
 	return f2fs_precache_extents(file_inode(filp));
 }
 
+/*
+ * This function is called when F2FS_IOC_ADD_ATOMIC_FILE command is called.
+ * It add file to atomic file set. Arguments *filp* and *arg* are file pointer
+ * and atomic file set key repectively. When this function is called,
+ * the file that is represented by *filp* is added to atomic file set that
+ * is represented by *arg*.
+ *
+ * If *arg* is NULL, it means there are no atomic file set that is already
+ * created. In this case, it create new atomic file set and add *filp* to
+ * new atomic file set. Because *arg* is double pointer to variable of user,
+ * user can get the address of atomic file set. The address of atomic file set
+ * is called key for atomic file set.
+ *
+ * We determined that f2fs_ioc_del_atomic_file () must be not implemented.
+ * It might generate empty atomic file set, and it could not be supervised
+ * because there is no object pointing that. 
+ * There is way to supervise all of atomic file set including empty one.
+ * However, it needs modification another kernel module like task_struct.
+ * We do not want modify kernel module but f2fs module.
+ *
+ * - Joontaek Oh.
+ */
+static int f2fs_ioc_add_atomic_file(struct file *filp, unsigned long arg)
+{
+	struct f2fs_sb_info *sbi;
+	struct f2fs_inode_info *fi;
+	struct atomic_file_set *afs;
+	struct atomic_file *af;
+	struct inode *inode;
+	bool allocated = false;
+	unsigned long key;
+
+	//if (!filp || !arg/* || if the *arg is user address, FAIL. */)
+	if (!filp /* || if the *arg is user address, FAIL. */) {
+		printk("[JATA DBG] %s return -ENOENT because filp is NULL\n", __func__);
+		return -ENOENT;
+	}
+
+	inode = filp->f_mapping->host;
+	fi = F2FS_I(inode);
+	sbi = F2FS_I_SB(inode);
+	//afs = *(struct atomic_file_set**)arg;
+	copy_from_user((void*)&key, (void*)arg, sizeof(struct atomic_file_set*));
+
+	if (f2fs_is_added_file(inode)) {
+		printk("[JATA DBG] %s return -ENOENT because file is already added\n", __func__);
+		return -ENOENT;
+	}
+
+	/*
+	 * If the value that being pointed by *arg* is NULL, we should allocate
+	 * new atomic file set.
+	 */
+	if (!key) {
+		static unsigned long key_allocator = 1;
+		int err;
+
+		afs = f2fs_kzalloc(sbi, sizeof(struct atomic_file_set), GFP_KERNEL);
+		if (!afs) {
+			printk("[JATA DBG] %s return -ENOMEM because afs couldn't be allocated\n", __func__);
+			return -ENOMEM;
+		}
+		//*(struct atomic_file_set**)arg = afs;
+		//copy_to_user((void*)arg, (void*)&afs, sizeof(arg));
+		afs->owner = current;
+		INIT_LIST_HEAD(&afs->afs_list);
+		INIT_LIST_HEAD(&afs->inmem_pages);
+		INIT_LIST_HEAD(&afs->inmem_node_pages);
+		init_rwsem(&afs->afs_rwsem);
+		afs->afs_magic = cpu_to_le32(F2FS_MUFIT_MAGIC);
+		allocated = true;
+		afs->key = key_allocator++;
+
+		err = rhashtable_insert_fast(&sbi->afs_ht, &afs->khtnode,
+		                             sbi->afs_kht_params);
+		if (err < 0) {
+			kfree(afs);
+			printk("[JATA DBG] %s return -EBUSY because insertion failed\n", __func__);
+			return -EBUSY;
+		}
+		copy_to_user((void*)arg, (void*)&afs->key, sizeof(afs->key));
+	} else {
+		afs = (struct atomic_file_set*)rhashtable_lookup_fast(&sbi->afs_ht, &key,
+		                                                      sbi->afs_kht_params);
+		if (!afs) {
+			printk("[JATA DBG] %s return -ENENT because lookup failed\n", __func__);
+			return -ENOENT;
+		}
+	}
+
+	if (le32_to_cpu(afs->afs_magic) != F2FS_MUFIT_MAGIC) {
+		printk("[JATA DBG] %s return -ENENT because magic value strange\n", __func__);
+		return -ENOENT;
+	}
+
+	af = f2fs_kzalloc(sbi, sizeof(struct atomic_file), GFP_KERNEL);
+
+	/* 
+	 * If afs has just been created, afs_list must be empty.
+	 * A empty afs_list have no way to be released, so there are
+	 * must no empty atomic file set having empty afs_list.
+	 * If this situation is occured, a empty afs should be
+	 * released.
+	 */
+	if (!af) {
+		if (allocated)
+			kfree(afs);
+		return -ENOMEM;
+	}
+
+	af->inode = inode;
+	af->afs = afs;
+	INIT_LIST_HEAD(&af->revoke_list);
+
+	/*
+	 * The file that is inserted to atomic file list at first,
+	 * should be last file.
+	 */
+	down_write(&afs->afs_rwsem);
+	if (allocated) {
+		af->last_file = true;
+		afs->last_file = af;
+	}
+
+	/*
+	 * Files are added to atomic file set in FILO policy.
+	 * But there is no out without calling f2fs_ioc_end_atomic_file_set ().
+	 */
+	list_add(&(af->list), &(afs->afs_list));
+	up_write(&afs->afs_rwsem);
+
+	inode_lock(inode);
+	fi->af = af;
+	set_inode_flag(inode, FI_ADDED_ATOMIC_FILE);
+	inode_unlock(inode);
+
+	return 0;
+}
+
+int f2fs_ioc_add_atomic_inode(struct inode *inode, unsigned long arg)
+{
+	struct f2fs_sb_info *sbi;
+	struct f2fs_inode_info *fi;
+	struct atomic_file_set *afs;
+	struct atomic_file *af;
+	bool allocated = false;
+
+	fi = F2FS_I(inode);
+	sbi = F2FS_I_SB(inode);
+	afs = *(struct atomic_file_set**)arg;
+
+	if (f2fs_is_added_file(inode))
+		return -ENOENT;
+
+	if (!afs) {
+		printk("[JATA DBG] (%s) No afs\n", __func__);
+		return -ENOENT;
+	}
+
+	if (le32_to_cpu(afs->afs_magic) != F2FS_MUFIT_MAGIC) {
+		printk("[JATA DBG] (%s) Invalid MAGIC\n", __func__);
+		return -ENOENT;
+	}
+
+	af = f2fs_kzalloc(sbi, sizeof(struct atomic_file), GFP_KERNEL);
+
+	/* 
+	 * If afs has just been created, afs_list must be empty.
+	 * A empty afs_list have no way to be released, so there are
+	 * must no empty atomic file set having empty afs_list.
+	 * If this situation is occured, a empty afs should be
+	 * released.
+	 */
+	if (!af) {
+		if (allocated)
+			kfree(afs);
+		printk("[JATA DBG] (%s) No af\n", __func__);
+		return -ENOMEM;
+	}
+
+	af->inode = inode;
+	af->afs = afs;
+	INIT_LIST_HEAD(&af->revoke_list);
+
+	/*
+	 * The file that is inserted to atomic file list at first,
+	 * should be last file.
+	 */
+	down_write(&afs->afs_rwsem);
+	if (!afs->last_file) {
+		af->last_file = true;
+		afs->last_file = af;
+	}
+
+	/*
+	 * Files are added to atomic file set in FILO policy.
+	 * But there is no out without calling f2fs_ioc_end_atomic_file_set ().
+	 */
+	list_add(&(af->list), &(afs->afs_list));
+	up_write(&afs->afs_rwsem);
+
+	/* We guaranteed that the caller alreay holds the inode lock. */
+	//inode_lock(inode);
+	fi->af = af;
+	set_inode_flag(inode, FI_ADDED_ATOMIC_FILE);
+	//inode_unlock(inode);
+
+	return 0;
+}
+
+/*
+ * This function is called when F2FS_IOC_START_ATOMIC_FILE_SET command is
+ * called. It set each file in atomic file set to atomic file. Atomic file
+ * is special file. When atomic file is changed, it makes not dirty page
+ * but inmem page. So atomic files do not have dirty pages. So it is not
+ * flushed to disk by pdflush. The *filp* is not used in this funciton.
+ *
+ * - Joontaek Oh.
+ */
+static int f2fs_ioc_start_atomic_file_set(struct file *filp, unsigned long arg)
+{
+	struct f2fs_sb_info *sbi;
+	struct atomic_file_set *afs;
+	struct atomic_file *af_elem, *tmp;
+	struct list_head *head;
+	struct inode *inode;
+
+	//afs = (struct atomic_file_set*)arg;
+
+	sbi = F2FS_I_SB(filp->f_inode);
+
+	if (!arg) {
+		if (!current->afs) {
+			afs = f2fs_kzalloc(sbi, sizeof(struct atomic_file_set), GFP_KERNEL);
+			if (!afs) {
+				printk("[JATA DBG] %s return -ENOMEM because afs couldn't be allocated\n", __func__);
+				return -ENOMEM;
+			}
+			afs->owner = current;
+			INIT_LIST_HEAD(&afs->afs_list);
+			INIT_LIST_HEAD(&afs->inmem_pages);
+			INIT_LIST_HEAD(&afs->inmem_node_pages);
+			init_rwsem(&afs->afs_rwsem);
+			afs->afs_magic = cpu_to_le32(F2FS_MUFIT_MAGIC);
+			afs->key = 0;
+
+			current->afs = afs;
+			current->is_atomic = true;
+		} else {
+			if (!current->is_atomic) {
+				printk("[JATA DBG] (%s) Current thread has atomic" \
+				       "file set but is not atomic thread", __func__);
+				return -ENOENT;
+			}
+		}
+		return 0;
+	} else {
+		afs = (struct atomic_file_set*)rhashtable_lookup_fast(&sbi->afs_ht, &arg,
+		                                                      sbi->afs_kht_params);
+		if (!afs) {
+			printk("[JATA DBG] %s return -ENOENT because lookup failed\n", __func__);
+			return -ENOENT;
+		}
+	}
+
+	if (le32_to_cpu(afs->afs_magic) != F2FS_MUFIT_MAGIC) {
+		printk("[JATA DBG] %s return -ENOENT because magic value strange\n", __func__);
+		return -ENOENT;
+	}
+
+	head = &afs->afs_list;
+
+	down_write(&afs->afs_rwsem);
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		/* 
+		 * Is there no any error? 
+		 * If af_elem is NULL? Huh?
+		 */
+		inode = af_elem->inode;
+		f2fs_ioc_start_atomic_write_inode(inode);
+	}
+	up_write(&afs->afs_rwsem);
+
+	return 0;
+}
+
+/*
+ * This function is called when F2FS_IOC_COMMIT_ATOMIC_FILE_SET command is
+ * called. It flush all files in atomic file set. The detail steps to flush
+ * all files in atomic file set is below.
+ *
+ * 1. Change all files in atomic file set to normal file from atomic file.
+ * 2. Flush all files in atomic file set in order.
+ * 3. Save the last node block address of each files in atomic file set to
+ *    mufit_node data in atomic_file_set structure.
+ * 4. When it flush last file in atomic file set, it allocate the new node
+ *    block, master node block. And save its node id to atomic_fiole_set
+ *    structure. If master node is already allocated, it use it. If master
+ *    node id in atomic_file_set structure have 0, it means master node is
+ *    unallocated yet. It is alright because the 0 of node id already
+ *    allocated to metadata.
+ *
+ * - Joontaek Oh.
+ */
+static int f2fs_ioc_commit_atomic_file_set(struct file *filp, unsigned long arg)
+{
+	struct f2fs_sb_info *sbi;
+	struct atomic_file_set *afs;
+	struct atomic_file *af_elem, *tmp;
+	struct inode *inode;
+	struct page *mpage;
+	struct list_head *head;
+	struct blk_plug plug;
+	int ret = 0;
+	struct writeback_control wbc = {
+		.sync_mode = WB_SYNC_ALL,
+		.nr_to_write = LONG_MAX,
+		.range_start = 0,
+		.range_end = LLONG_MAX,
+		.for_reclaim = 0,
+	};
+	struct inmem_pages *inmem_cur, *inmem_tmp;
+	struct inmem_node_pages *inmem_node_cur, *inmem_node_tmp;
+	struct f2fs_io_info fio;
+	pgoff_t last_idx = ULONG_MAX;
+	
+	sbi = F2FS_I_SB(filp->f_inode);
+
+	if (!arg && current->is_atomic)
+		afs = current->afs;
+	else
+		afs = (struct atomic_file_set*)rhashtable_lookup_fast(&sbi->afs_ht, &arg,
+		                                                      sbi->afs_kht_params);
+
+	if (!afs || le32_to_cpu(afs->afs_magic) != F2FS_MUFIT_MAGIC) {
+		printk("[JATA DBG] %s return -ENOENT because lookup failed\n", __func__);
+		return -ENOENT;
+	}
+
+	if (!afs->last_file)
+		return 0;
+
+	fio.sbi = sbi;
+	fio.type = DATA;
+	fio.op = REQ_OP_WRITE;
+	fio.op_flags = REQ_SYNC | REQ_PRIO;
+	fio.io_type = FS_DATA_IO;
+	fio.io_wbc = NULL;
+
+	head = &afs->afs_list;
+
+	down_write(&afs->afs_rwsem);
+
+	/* checkpoint should be blocked before below code block */
+	f2fs_balance_fs(sbi, true);
+	f2fs_lock_op(sbi);
+
+	if (!afs->master_nid) {
+		ret = f2fs_build_master_node(afs);
+		if (ret) {
+			printk("[JATA DBG] %s return error because building master node failed\n", __func__);
+			return ret;
+		}
+	}
+
+	mpage = f2fs_get_node_page(sbi, afs->master_nid);
+
+	/*
+	 * This loop is for processing inmem_pages list per
+	 * inode.
+	 * - Joontaek Oh.
+	 */
+	/*list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode;
+		inode = af_elem->inode;
+
+		inode_lock(inode);
+		down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		f2fs_commit_inmem_pages_atomic_file_set(inode, &af_elem->revoke_list);
+		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		inode_unlock(inode);
+	}*/
+
+	/*
+	 * Below codes are for processing inmem_pages list
+	 * per atomic file group. These code is divided code
+	 * of above loop which is for processing inmem_pages
+	 * list per inode.
+	 * These code is consist of three steps.
+	 *
+	 * 1. Prepare atomic write of data pages:
+	 *    Get the inode lock and other semaphore or
+	 *    set the inode flag FI_ATOMIC_COMMIT.
+	 * 2. Do atomic write of data pages:
+	 *    Walk inmem_pages list in atomic file group
+	 *    and make them into one request in plug list.
+	 * 3. Release lock
+	 * - Joontaek Oh.
+	 */
+
+
+	blk_start_plug(&plug);
+
+	/*
+	 * Step 1: Prepare atomic write of data pages
+	 */
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode = af_elem->inode;
+		struct f2fs_inode_info *fi = F2FS_I(inode);
+		inode_lock(inode);
+		down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		mutex_lock(&fi->inmem_lock);
+		set_inode_flag(inode, FI_ATOMIC_COMMIT);
+	}
+
+	/*
+	 * Step 2: Do atomic write of data pages
+	 */
+	list_for_each_entry_safe(inmem_cur, inmem_tmp, &afs->inmem_pages, list) {
+		struct page *page = inmem_cur->page;
+
+		inode = page->mapping->host;
+		fio.ino = inode->i_ino;
+
+		lock_page(page);
+		if (page->mapping == inode->i_mapping) {
+			set_page_dirty(page);
+			f2fs_wait_on_page_writeback(page, DATA, true);
+			if (clear_page_dirty_for_io(page)) {
+				inode_dec_dirty_pages(inode);
+				f2fs_remove_dirty_inode(inode);
+			}
+retry:
+			fio.page = page;
+			fio.old_blkaddr = NULL_ADDR;
+			fio.encrypted_page = NULL;
+			fio.need_lock = LOCK_DONE;
+			ret = f2fs_do_write_data_page(&fio);
+			if (ret) {
+				if (ret == -ENOMEM) {
+					congestion_wait(BLK_RW_ASYNC, HZ/50);
+					cond_resched();
+					goto retry;
+				}
+				unlock_page(page);
+				break;
+			}
+			inmem_cur->old_addr = fio.old_blkaddr;
+			last_idx = page->index;
+		}
+		unlock_page(page);
+		list_move_tail(&inmem_cur->list, &F2FS_I(inode)->af->revoke_list);
+	}
+
+	/*
+	 * Step 3: Release lock
+	 */
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode = af_elem->inode;
+		struct f2fs_inode_info *fi = F2FS_I(inode);
+
+		spin_lock(&sbi->inode_lock[ATOMIC_FILE]);
+		if (!list_empty(&fi->inmem_ilist))
+			list_del_init(&fi->inmem_ilist);
+		spin_unlock(&sbi->inode_lock[ATOMIC_FILE]);
+
+		mutex_unlock(&fi->inmem_lock);
+		clear_inode_flag(inode, FI_ATOMIC_COMMIT);
+		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		inode_unlock(inode);
+	}
+
+	/*
+	 * This code is for submitting node.
+	 * It walks files in atomic file group, and call
+	 * f2fs_fsync_node_pages() for each file.
+	 * - Joontaek Oh.
+	 */
+	/*list_for_each_entry_safe(af_elem, tmp, head, list) {
+		inode = af_elem->inode;
+		f2fs_fsync_node_pages(sbi, inode, &wbc, true);
+	}*/
+
+	/*
+	 * This code is for sumitting node also.
+	 * But it walk only modified node list (afs->inmem_node_pages).
+	 * - Joontaek Oh.
+	 */
+	list_for_each_entry_safe(inmem_node_cur, inmem_node_tmp, &afs->inmem_node_pages, list) {
+		struct page *page = inmem_node_cur->page;
+
+		lock_page(page);
+
+		f2fs_wait_on_page_writeback(page, NODE, true);
+
+		set_fsync_mark(page, 0);
+		set_dentry_mark(page, 0);
+
+		if (!clear_page_dirty_for_io(page)) {
+			unlock_page(page);
+			continue;
+		}
+
+		ret = ____write_node_page(page, false, NULL, &wbc, false, FS_NODE_IO);
+
+		if (ret) {
+			unlock_page(page);
+			return -1;
+		}
+
+		if (IS_INODE(page)) {
+			struct master_node *mn;
+			struct node_info ni;
+			nid_t nid;
+
+			mn = page_address(mpage);
+
+			nid = nid_of_node(page);
+			f2fs_get_node_info(sbi, nid, &ni);
+
+			mn->atm_addrs[afs->commit_file_count++] = ni.blk_addr;
+		}
+
+		set_page_private(page, 0);
+		ClearPagePrivate(page);
+		list_del(&inmem_node_cur->list);
+		kmem_cache_free(inmem_entry_slab, inmem_node_cur);
+	}
+	f2fs_flush_merged_writes(sbi);
+	blk_finish_plug(&plug);
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode;
+		inode = af_elem->inode;
+		____revoke_inmem_pages(inode, &af_elem->revoke_list, false, false);
+	}
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode;
+		inode = af_elem->inode;
+		f2fs_wait_on_node_pages_writeback(sbi, inode->i_ino);
+	}
+
+	set_fsync_mark(mpage, 1);
+	set_page_dirty(mpage);
+	f2fs_wait_on_page_writeback(mpage, NODE, true);
+
+	if (!clear_page_dirty_for_io(mpage)) {
+		printk("[JATA DBG] (%s) clear master node page fails\n", __func__);
+		goto out;
+	}
+
+	if (____write_node_page(mpage, true, NULL, &wbc, false, FS_NODE_IO)) {
+		printk("[JATA DBG] (%s) master node page write fails\n", __func__);
+		unlock_page(mpage);
+	}
+	f2fs_wait_on_page_writeback(mpage, NODE, true);
+
+out:
+	f2fs_put_page(mpage, 0);
+	afs->commit_file_count = 0;
+
+	/* checkpoint should be unblocked now. */
+	f2fs_unlock_op(sbi);
+
+	up_write(&afs->afs_rwsem);
+
+	if (current->is_atomic && current->afs == afs) {
+		f2fs_ioc_end_atomic_file_set_thread(NULL, afs);
+		current->is_atomic = false;
+		current->afs = NULL;
+	}
+
+	return 0;
+}
+
+static int f2fs_ioc_commit_atomic_file_set_noflush(struct file *filp, unsigned long arg)
+{
+	struct f2fs_sb_info *sbi;
+	struct atomic_file_set *afs;
+	struct atomic_file *af_elem, *tmp;
+	struct inode *inode;
+	struct page *mpage;
+	struct list_head *head;
+	struct blk_plug plug;
+	int ret = 0;
+	struct writeback_control wbc = {
+		.sync_mode = WB_SYNC_ALL,
+		.nr_to_write = LONG_MAX,
+		.range_start = 0,
+		.range_end = LLONG_MAX,
+		.for_reclaim = 0,
+	};
+
+	sbi = F2FS_I_SB(filp->f_inode);
+
+	if (!arg && current->is_atomic)
+		afs = current->afs;
+	else
+		afs = (struct atomic_file_set*)rhashtable_lookup_fast(&sbi->afs_ht, &arg,
+		                                                      sbi->afs_kht_params);
+
+	if (!afs || le32_to_cpu(afs->afs_magic) != F2FS_MUFIT_MAGIC)
+		return -ENOENT;
+
+	head = &afs->afs_list;
+
+	down_write(&afs->afs_rwsem);
+
+	/* checkpoint should be blocked before below code block */
+
+	if (!afs->master_nid) {
+		ret = f2fs_build_master_node(afs);
+		if (ret)
+			return ret;
+	}
+
+	blk_start_plug(&plug);
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode;
+		inode = af_elem->inode;
+
+		inode_lock(inode);
+		down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		f2fs_commit_inmem_pages_atomic_file_set(inode, &af_elem->revoke_list);
+		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		inode_unlock(inode);
+	}
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		inode = af_elem->inode;
+		f2fs_fsync_node_pages(sbi, inode, &wbc, true);
+	}
+
+	blk_finish_plug(&plug);
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode;
+		inode = af_elem->inode;
+		____revoke_inmem_pages(inode, &af_elem->revoke_list, false, false);
+		f2fs_wait_on_node_pages_writeback(sbi, inode->i_ino);
+	}
+
+	mpage = f2fs_get_node_page(sbi, afs->master_nid);
+	set_fsync_mark(mpage, 1);
+	set_page_dirty(mpage);
+	f2fs_wait_on_page_writeback(mpage, NODE, true);
+
+	if (!clear_page_dirty_for_io(mpage)) {
+		printk("[JATA DBG] (%s) clear master node page fails\n", __func__);
+		goto out;
+	}
+
+	if (____write_node_page(mpage, false, NULL, &wbc, true, FS_NODE_IO)) {
+		printk("[JATA DBG] (%s) master node page write fails\n", __func__);
+		unlock_page(mpage);
+	}
+	f2fs_wait_on_page_writeback(mpage, NODE, true);
+
+	f2fs_issue_flush(sbi, afs->last_file->inode->i_ino);
+out:
+	f2fs_put_page(mpage, 0);
+	afs->commit_file_count = 0;
+
+	/* checkpoint should be unblocked now. */
+
+	up_write(&afs->afs_rwsem);
+
+	if (current->is_atomic && current->afs == afs) {
+		f2fs_ioc_end_atomic_file_set(NULL, (unsigned long)afs);
+		current->is_atomic = false;
+	}
+
+	return 0;
+}
+
+static int f2fs_ioc_commit_atomic_file_set_nodma(struct file *filp, unsigned long arg)
+{
+	struct f2fs_sb_info *sbi;
+	struct atomic_file_set *afs;
+	struct atomic_file *af_elem, *tmp;
+	struct inode *inode;
+	struct page *mpage;
+	struct list_head *head;
+	struct blk_plug plug;
+	int ret = 0;
+	struct writeback_control wbc = {
+		.sync_mode = WB_SYNC_ALL,
+		.nr_to_write = LONG_MAX,
+		.range_start = 0,
+		.range_end = LLONG_MAX,
+		.for_reclaim = 0,
+	};
+
+	sbi = F2FS_I_SB(filp->f_inode);
+
+	if (!arg && current->is_atomic)
+		afs = current->afs;
+	else
+		afs = (struct atomic_file_set*)rhashtable_lookup_fast(&sbi->afs_ht, &arg,
+		                                                      sbi->afs_kht_params);
+
+	if (!afs || le32_to_cpu(afs->afs_magic) != F2FS_MUFIT_MAGIC)
+		return -ENOENT;
+
+	head = &afs->afs_list;
+
+	down_write(&afs->afs_rwsem);
+
+	/* checkpoint should be blocked before below code block */
+
+	if (!afs->master_nid) {
+		ret = f2fs_build_master_node(afs);
+		if (ret)
+			return ret;
+	}
+
+	blk_start_plug(&plug);
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode;
+		inode = af_elem->inode;
+
+		inode_lock(inode);
+		down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		f2fs_commit_inmem_pages_atomic_file_set(inode, &af_elem->revoke_list);
+		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		inode_unlock(inode);
+	}
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		inode = af_elem->inode;
+		f2fs_fsync_node_pages(sbi, inode, &wbc, true);
+	}
+
+	blk_finish_plug(&plug);
+
+	mpage = f2fs_get_node_page(sbi, afs->master_nid);
+	set_fsync_mark(mpage, 1);
+	set_page_dirty(mpage);
+	f2fs_wait_on_page_writeback(mpage, NODE, true);
+
+	if (!clear_page_dirty_for_io(mpage)) {
+		printk("[JATA DBG] (%s) clear master node page fails\n", __func__);
+		goto out;
+	}
+
+	if (____write_node_page(mpage, true, NULL, &wbc, true, FS_NODE_IO)) {
+		printk("[JATA DBG] (%s) master node page write fails\n", __func__);
+		unlock_page(mpage);
+	}
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode;
+		inode = af_elem->inode;
+		____revoke_inmem_pages(inode, &af_elem->revoke_list, false, false);
+		f2fs_wait_on_node_pages_writeback(sbi, inode->i_ino);
+	}
+
+	f2fs_wait_on_page_writeback(mpage, NODE, true);
+out:
+	f2fs_put_page(mpage, 0);
+	afs->commit_file_count = 0;
+
+	/* checkpoint should be unblocked now. */
+
+	up_write(&afs->afs_rwsem);
+
+	if (current->is_atomic && current->afs == afs) {
+		f2fs_ioc_end_atomic_file_set(NULL, (unsigned long)afs);
+		current->is_atomic = false;
+	}
+
+	return 0;
+}
+
+static int f2fs_ioc_commit_atomic_file_set_noflushdma(struct file *filp, unsigned long arg)
+{
+	struct f2fs_sb_info *sbi;
+	struct atomic_file_set *afs;
+	struct atomic_file *af_elem, *tmp;
+	struct inode *inode;
+	struct page *mpage;
+	struct list_head *head;
+	struct blk_plug plug;
+	int ret = 0;
+	struct writeback_control wbc = {
+		.sync_mode = WB_SYNC_ALL,
+		.nr_to_write = LONG_MAX,
+		.range_start = 0,
+		.range_end = LLONG_MAX,
+		.for_reclaim = 0,
+	};
+
+	sbi = F2FS_I_SB(filp->f_inode);
+
+	if (!arg && current->is_atomic)
+		afs = current->afs;
+	else
+		afs = (struct atomic_file_set*)rhashtable_lookup_fast(&sbi->afs_ht, &arg,
+		                                                      sbi->afs_kht_params);
+
+	if (!afs || le32_to_cpu(afs->afs_magic) != F2FS_MUFIT_MAGIC)
+		return -ENOENT;
+
+	head = &afs->afs_list;
+
+	down_write(&afs->afs_rwsem);
+
+	/* checkpoint should be blocked before below code block */
+
+	if (!afs->master_nid) {
+		ret = f2fs_build_master_node(afs);
+		if (ret)
+			return ret;
+	}
+
+	blk_start_plug(&plug);
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode;
+		inode = af_elem->inode;
+
+		inode_lock(inode);
+		down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		f2fs_commit_inmem_pages_atomic_file_set(inode, &af_elem->revoke_list);
+		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		inode_unlock(inode);
+	}
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		inode = af_elem->inode;
+		f2fs_fsync_node_pages(sbi, inode, &wbc, true);
+	}
+
+	blk_finish_plug(&plug);
+
+	mpage = f2fs_get_node_page(sbi, afs->master_nid);
+	set_fsync_mark(mpage, 1);
+	set_page_dirty(mpage);
+	f2fs_wait_on_page_writeback(mpage, NODE, true);
+
+	if (!clear_page_dirty_for_io(mpage)) {
+		printk("[JATA DBG] (%s) clear master node page fails\n", __func__);
+		goto out;
+	}
+
+	if (____write_node_page(mpage, false, NULL, &wbc, true, FS_NODE_IO)) {
+		printk("[JATA DBG] (%s) master node page write fails\n", __func__);
+		unlock_page(mpage);
+	}
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		struct inode *inode;
+		inode = af_elem->inode;
+		____revoke_inmem_pages(inode, &af_elem->revoke_list, false, false);
+		f2fs_wait_on_node_pages_writeback(sbi, inode->i_ino);
+	}
+
+	f2fs_wait_on_page_writeback(mpage, NODE, true);
+
+	f2fs_issue_flush(sbi, afs->last_file->inode->i_ino);
+out:
+	f2fs_put_page(mpage, 0);
+	afs->commit_file_count = 0;
+
+	/* checkpoint should be unblocked now. */
+
+	up_write(&afs->afs_rwsem);
+
+	if (current->is_atomic && current->afs == afs) {
+		f2fs_ioc_end_atomic_file_set(NULL, (unsigned long)afs);
+		current->is_atomic = false;
+	}
+
+	return 0;
+}
+
+/*
+ * This function is called when F2FS_IOC_END_ATOMIC_FILE_SET command is
+ * called. It deallocate the atomic file set including atomic_file_set
+ * structure, atomic_file structure and master node.
+ *
+ * - Joontaek Oh.
+ */
+static int f2fs_ioc_end_atomic_file_set(struct file *filp, unsigned long arg)
+{
+	struct atomic_file_set *afs;
+	struct atomic_file *af_elem, *tmp;
+	struct list_head *head;
+	struct f2fs_sb_info *sbi;
+
+	sbi = F2FS_I_SB(filp->f_inode);
+
+	afs = (struct atomic_file_set*)rhashtable_lookup_fast(&sbi->afs_ht, &arg,
+	                                                      sbi->afs_kht_params);
+
+	if (!afs || afs->afs_magic != cpu_to_le32(F2FS_MUFIT_MAGIC))
+		return -ENOENT;
+
+	rhashtable_remove_fast(&sbi->afs_ht, &afs->khtnode, sbi->afs_kht_params);
+
+	head = &afs->afs_list;
+
+	down_write(&afs->afs_rwsem);
+
+	if (afs->master_nid)
+		f2fs_truncate_master_node(afs);
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		/* Is there no any process to do when release atomic_file? */
+		F2FS_I(af_elem->inode)->af = NULL;
+		clear_inode_flag(af_elem->inode, FI_ADDED_ATOMIC_FILE);
+		list_del(&af_elem->list);
+		kfree(af_elem);
+	}
+	
+	up_write(&afs->afs_rwsem);
+
+	kfree(afs);
+
+	return 0;
+}
+
+static int f2fs_ioc_end_atomic_file_set_thread(struct file *filp, struct atomic_file_set *afs)
+{
+	struct atomic_file *af_elem, *tmp;
+	struct list_head *head;
+	struct f2fs_sb_info *sbi;
+
+	sbi = F2FS_I_SB(filp->f_inode);
+
+	if (!afs || afs->afs_magic != cpu_to_le32(F2FS_MUFIT_MAGIC))
+		return -ENOENT;
+
+	head = &afs->afs_list;
+
+	down_write(&afs->afs_rwsem);
+
+	if (afs->master_nid)
+		f2fs_truncate_master_node(afs);
+
+	list_for_each_entry_safe(af_elem, tmp, head, list) {
+		/* Is there no any process to do when release atomic_file? */
+		F2FS_I(af_elem->inode)->af = NULL;
+		clear_inode_flag(af_elem->inode, FI_ADDED_ATOMIC_FILE);
+		list_del(&af_elem->list);
+		kfree(af_elem);
+	}
+	
+	up_write(&afs->afs_rwsem);
+
+	kfree(afs);
+
+	return 0;
+}
+
 long f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 {
 	if (unlikely(f2fs_cp_error(F2FS_I_SB(file_inode(filp)))))
@@ -2866,7 +3890,7 @@
 	case F2FS_IOC_START_ATOMIC_WRITE:
 		return f2fs_ioc_start_atomic_write(filp);
 	case F2FS_IOC_COMMIT_ATOMIC_WRITE:
-		return f2fs_ioc_commit_atomic_write(filp);
+		return f2fs_ioc_commit_atomic_write(filp, arg);
 	case F2FS_IOC_START_VOLATILE_WRITE:
 		return f2fs_ioc_start_volatile_write(filp);
 	case F2FS_IOC_RELEASE_VOLATILE_WRITE:
@@ -2907,6 +3931,20 @@
 		return f2fs_ioc_set_pin_file(filp, arg);
 	case F2FS_IOC_PRECACHE_EXTENTS:
 		return f2fs_ioc_precache_extents(filp, arg);
+	case F2FS_IOC_ADD_ATOMIC_FILE:
+		return f2fs_ioc_add_atomic_file(filp, arg);
+	case F2FS_IOC_START_ATOMIC_FILE_SET:
+		return f2fs_ioc_start_atomic_file_set(filp, arg);
+	case F2FS_IOC_COMMIT_ATOMIC_FILE_SET:
+		return f2fs_ioc_commit_atomic_file_set(filp, arg);
+	case F2FS_IOC_END_ATOMIC_FILE_SET:
+		return f2fs_ioc_end_atomic_file_set(filp, arg);
+	case F2FS_IOC_COMMIT_NOFLUSH_ATOMIC_FILE_SET:
+		return f2fs_ioc_commit_atomic_file_set_noflush(filp, arg);
+	case F2FS_IOC_COMMIT_NODMA_ATOMIC_FILE_SET:
+		return f2fs_ioc_commit_atomic_file_set_nodma(filp, arg);
+	case F2FS_IOC_COMMIT_NOFLUSHDMA_ATOMIC_FILE_SET:
+		return f2fs_ioc_commit_atomic_file_set_noflushdma(filp, arg);
 	default:
 		return -ENOTTY;
 	}
diff -urN original/fs/f2fs/gc.c linux/fs/f2fs/gc.c
--- original/fs/f2fs/gc.c	2020-08-18 21:43:20.175705250 +0900
+++ linux/fs/f2fs/gc.c	2020-12-01 23:35:51.874283363 +0900
@@ -1046,6 +1046,7 @@
 				prefree_segments(sbi));
 
 	cpc.reason = __get_cp_reason(sbi);
+
 gc_more:
 	if (unlikely(!(sbi->sb->s_flags & SB_ACTIVE))) {
 		ret = -EINVAL;
@@ -1127,6 +1128,7 @@
 
 	if (sync)
 		ret = sec_freed ? 0 : -EAGAIN;
+
 	return ret;
 }
 
diff -urN original/fs/f2fs/node.c linux/fs/f2fs/node.c
--- original/fs/f2fs/node.c	2020-08-18 21:43:20.175705250 +0900
+++ linux/fs/f2fs/node.c	2020-12-01 23:35:25.950857481 +0900
@@ -1431,6 +1431,13 @@
 	return AOP_WRITEPAGE_ACTIVATE;
 }
 
+int ____write_node_page(struct page *page, bool atomic, bool *submitted,
+				struct writeback_control *wbc, bool do_balance,
+				enum iostat_type io_type)
+{
+	return __write_node_page(page, atomic, submitted, wbc, do_balance, io_type);
+}
+
 void f2fs_move_node_page(struct page *node_page, int gc_type)
 {
 	if (gc_type == FG_GC) {
@@ -1484,6 +1491,8 @@
 		last_page = last_fsync_dnode(sbi, ino);
 		if (IS_ERR_OR_NULL(last_page))
 			return PTR_ERR_OR_ZERO(last_page);
+		if (f2fs_is_added_file(inode))
+			atomic = false;
 	}
 retry:
 	pagevec_init(&pvec);
@@ -1508,6 +1517,10 @@
 				continue;
 			if (ino_of_node(page) != ino)
 				continue;
+			if (is_master_node(page)) {
+				marked = true;
+				continue;
+			}
 
 			lock_page(page);
 
@@ -1530,7 +1543,7 @@
 			set_fsync_mark(page, 0);
 			set_dentry_mark(page, 0);
 
-			if (!atomic || page == last_page) {
+			if ((!atomic || page == last_page)) {
 				set_fsync_mark(page, 1);
 				if (IS_INODE(page)) {
 					if (is_inode_flag_set(inode,
@@ -1560,7 +1573,26 @@
 			}
 
 			if (page == last_page) {
+				if (f2fs_is_added_file(inode)) {
+					struct atomic_file_set *afs = F2FS_I(inode)->af->afs;
+					struct master_node *mn;
+					struct page *mpage;
+					struct node_info ni;
+					nid_t nid;
+
+					mpage = f2fs_get_node_page(sbi, afs->master_nid);
+					mn = page_address(mpage);
+
+					/* Copy Node address to Master node block  */
+					nid = nid_of_node(page);
+					f2fs_get_node_info(sbi, nid, &ni);
+
+					mn->atm_addrs[afs->commit_file_count++] = ni.blk_addr;
+					f2fs_put_page(mpage, 1);
+				}
+
 				f2fs_put_page(page, 0);
+
 				marked = true;
 				break;
 			}
@@ -2909,3 +2941,62 @@
 	kmem_cache_destroy(free_nid_slab);
 	kmem_cache_destroy(nat_entry_slab);
 }
+
+/*
+ * It build new master node. Master node is special node. It is used with Multi-File 
+ * Transactional write. It is allocated when atomic file set having no master node is
+ * committed. When each file in atomic file set are committed, block address of its
+ * last node block is added to master node block. 
+ *
+ * - Joontaek Oh.
+ */
+int f2fs_build_master_node(struct atomic_file_set *afs)
+{
+	struct f2fs_sb_info *sbi;
+	struct inode *inode;
+	struct dnode_of_data dn;
+	struct page *mpage;
+	nid_t new_nid;
+
+	inode = afs->last_file->inode;
+	sbi = F2FS_I_SB(inode);
+
+	if (!f2fs_alloc_nid(sbi, &new_nid))
+		return -ENOSPC;
+	set_new_dnode(&dn, inode, NULL, NULL, new_nid);
+	mpage = f2fs_new_node_page(&dn, MASTER_NODE_OFFSET);
+	if (IS_ERR(mpage)) {
+		f2fs_alloc_nid_failed(sbi, new_nid);
+		return PTR_ERR(mpage);
+
+	}
+	f2fs_alloc_nid_done(sbi, new_nid);
+
+	afs->master_nid = new_nid;
+
+	//clear_node_page_dirty(mpage);
+
+	f2fs_put_page(mpage, 1);
+	return 0;
+}
+
+int f2fs_truncate_master_node(struct atomic_file_set *afs)
+{
+	struct f2fs_sb_info *sbi;
+	struct inode *inode;
+	struct dnode_of_data dn;
+	struct page *mpage;
+
+	inode = afs->last_file->inode;
+	sbi = F2FS_I_SB(inode);
+
+	mpage = f2fs_get_node_page(sbi, afs->master_nid);
+	if (IS_ERR(mpage))
+		return PTR_ERR(mpage);
+
+	/* free master node */
+	set_new_dnode(&dn, inode, NULL, mpage, afs->master_nid);
+	truncate_node(&dn);
+	afs->master_nid = 0;
+	return 0;
+}
diff -urN original/fs/f2fs/node.h linux/fs/f2fs/node.h
--- original/fs/f2fs/node.h	2020-08-18 21:43:20.175705250 +0900
+++ linux/fs/f2fs/node.h	2020-09-25 02:50:01.034776570 +0900
@@ -447,3 +447,8 @@
 }
 #define set_dentry_mark(page, mark)	set_mark(page, mark, DENT_BIT_SHIFT)
 #define set_fsync_mark(page, mark)	set_mark(page, mark, FSYNC_BIT_SHIFT)
+
+static inline int is_master_node(struct page *page)
+{
+	return ofs_of_node(page) == MASTER_NODE_OFFSET;
+}
diff -urN original/fs/f2fs/recovery.c linux/fs/f2fs/recovery.c
--- original/fs/f2fs/recovery.c	2020-08-18 21:43:20.175705250 +0900
+++ linux/fs/f2fs/recovery.c	2020-09-25 02:50:01.030776672 +0900
@@ -243,6 +243,9 @@
 	unsigned int loop_cnt = 0;
 	unsigned int free_blocks = sbi->user_block_count -
 					valid_user_blocks(sbi);
+	//struct page *master_page = NULL;
+	//struct master_node *mn = NULL;
+	//int afs_files = 0;
 	int err = 0;
 
 	/* get node pages in the current segment */
@@ -263,6 +266,15 @@
 		if (!is_fsync_dnode(page))
 			goto next;
 
+		if (is_master_node(page)) {
+			goto next;
+			/*master_page = page;
+			mn = &F2FS_NODE(master_page)->mn;
+			afs_files = mn->count_valid_addr;
+read_master:
+			page = f2fs_get_tmp_page(sbi, mn->atm_addrs[mn->count_valid_addr]);*/
+		}
+
 		entry = get_fsync_inode(head, ino_of_node(page));
 		if (!entry) {
 			bool quota_inode = false;
@@ -294,6 +306,12 @@
 
 		if (IS_INODE(page) && is_dent_dnode(page))
 			entry->last_dentry = blkaddr;
+
+		/*if (afs_files)
+			goto read_master;
+
+		if (master_page)
+			page = master_page;*/
 next:
 		/* sanity check in order to detect looped node chain */
 		if (++loop_cnt >= free_blocks ||
diff -urN original/fs/f2fs/segment.c linux/fs/f2fs/segment.c
--- original/fs/f2fs/segment.c	2020-08-18 21:43:20.175705250 +0900
+++ linux/fs/f2fs/segment.c	2020-09-25 02:50:01.046776262 +0900
@@ -31,7 +31,8 @@
 static struct kmem_cache *discard_entry_slab;
 static struct kmem_cache *discard_cmd_slab;
 static struct kmem_cache *sit_entry_set_slab;
-static struct kmem_cache *inmem_entry_slab;
+//static struct kmem_cache *inmem_entry_slab;
+struct kmem_cache *inmem_entry_slab;
 
 static unsigned long __reverse_ulong(unsigned char *str)
 {
@@ -190,6 +191,13 @@
 	struct f2fs_inode_info *fi = F2FS_I(inode);
 	struct inmem_pages *new;
 
+	/*
+	if (inode) {
+		struct dentry *dentry = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);
+		if (dentry)
+			printk("[JATA DBG] (%s) %u: %s\n", __func__, current->pid, dentry->d_name.name);
+	}*/
+
 	f2fs_trace_pid(page);
 
 	set_page_private(page, (unsigned long)ATOMIC_WRITTEN_PAGE);
@@ -204,7 +212,17 @@
 	/* increase reference count with clean state */
 	mutex_lock(&fi->inmem_lock);
 	get_page(page);
-	list_add_tail(&new->list, &fi->inmem_pages);
+	if (f2fs_is_added_file(inode)) {
+		if (!fi->af || !fi->af->afs) {
+			printk("[JATA DBG] (%s) atomic file group"
+			       "should no be NULL\n", __func__);
+			return;
+		}
+		down_write(&fi->af->afs->afs_rwsem);
+		list_add_tail(&new->list, &fi->af->afs->inmem_pages);
+		up_write(&fi->af->afs->afs_rwsem);
+	} else
+		list_add_tail(&new->list, &fi->inmem_pages);
 	spin_lock(&sbi->inode_lock[ATOMIC_FILE]);
 	if (list_empty(&fi->inmem_ilist))
 		list_add_tail(&fi->inmem_ilist, &sbi->inode_list[ATOMIC_FILE]);
@@ -274,6 +292,12 @@
 	return err;
 }
 
+int ____revoke_inmem_pages(struct inode *inode,
+				struct list_head *head, bool drop, bool recover)
+{
+	return __revoke_inmem_pages(inode, head, drop, recover);
+}
+
 void f2fs_drop_inmem_pages_all(struct f2fs_sb_info *sbi, bool gc_failure)
 {
 	struct list_head *head = &sbi->inode_list[ATOMIC_FILE];
@@ -409,7 +433,7 @@
 		list_move_tail(&cur->list, &revoke_list);
 	}
 
-	if (last_idx != ULONG_MAX)
+	if (last_idx != ULONG_MAX && !f2fs_is_added_file(inode))
 		f2fs_submit_merged_write_cond(sbi, inode, 0, last_idx, DATA);
 
 	if (err) {
@@ -432,6 +456,63 @@
 	return err;
 }
 
+static int __f2fs_commit_inmem_pages_atomic_file_set(struct inode *inode, struct list_head *revoke_list)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct inmem_pages *cur, *tmp;
+	struct f2fs_io_info fio = {
+		.sbi = sbi,
+		.ino = inode->i_ino,
+		.type = DATA,
+		.op = REQ_OP_WRITE,
+		.op_flags = REQ_SYNC | REQ_PRIO,
+		.io_type = FS_DATA_IO,
+	};
+	pgoff_t last_idx = ULONG_MAX;
+	int err = 0;
+
+	INIT_LIST_HEAD(revoke_list);
+
+	list_for_each_entry_safe(cur, tmp, &fi->inmem_pages, list) {
+		struct page *page = cur->page;
+
+		lock_page(page);
+		if (page->mapping == inode->i_mapping) {
+			trace_f2fs_commit_inmem_page(page, INMEM);
+
+			set_page_dirty(page);
+			f2fs_wait_on_page_writeback(page, DATA, true);
+			if (clear_page_dirty_for_io(page)) {
+				inode_dec_dirty_pages(inode);
+				f2fs_remove_dirty_inode(inode);
+			}
+retry:
+			fio.page = page;
+			fio.old_blkaddr = NULL_ADDR;
+			fio.encrypted_page = NULL;
+			fio.need_lock = LOCK_DONE;
+			err = f2fs_do_write_data_page(&fio);
+			if (err) {
+				if (err == -ENOMEM) {
+					congestion_wait(BLK_RW_ASYNC, HZ/50);
+					cond_resched();
+					goto retry;
+				}
+				unlock_page(page);
+				break;
+			}
+			/* record old blkaddr for revoking */
+			cur->old_addr = fio.old_blkaddr;
+			last_idx = page->index;
+		}
+		unlock_page(page);
+		list_move_tail(&cur->list, revoke_list);
+	}
+
+	return err;
+}
+
 int f2fs_commit_inmem_pages(struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -458,6 +539,33 @@
 	return err;
 }
 
+int f2fs_commit_inmem_pages_atomic_file_set(struct inode *inode, struct list_head *revoke_list)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	int err;
+
+	f2fs_balance_fs(sbi, true);
+	f2fs_lock_op(sbi);
+
+	set_inode_flag(inode, FI_ATOMIC_COMMIT);
+
+	mutex_lock(&fi->inmem_lock);
+	err = __f2fs_commit_inmem_pages_atomic_file_set(inode, revoke_list);
+
+	spin_lock(&sbi->inode_lock[ATOMIC_FILE]);
+	if (!list_empty(&fi->inmem_ilist))
+		list_del_init(&fi->inmem_ilist);
+	spin_unlock(&sbi->inode_lock[ATOMIC_FILE]);
+	mutex_unlock(&fi->inmem_lock);
+
+	clear_inode_flag(inode, FI_ATOMIC_COMMIT);
+
+	f2fs_unlock_op(sbi);
+	return err;
+}
+
+
 /*
  * This function balances dirty node and dentry pages.
  * In addition, it controls garbage collection.
@@ -2981,6 +3089,7 @@
 
 		f2fs_submit_merged_write_cond(sbi, page->mapping->host,
 						0, page->index, type);
+
 		if (ordered)
 			wait_on_page_writeback(page);
 		else
diff -urN original/fs/f2fs/segment.h linux/fs/f2fs/segment.h
--- original/fs/f2fs/segment.h	2020-08-18 21:43:20.175705250 +0900
+++ linux/fs/f2fs/segment.h	2020-09-25 02:50:01.034776570 +0900
@@ -223,6 +223,13 @@
 	block_t old_addr;		/* for revoking when fail to commit */
 };
 
+struct inmem_node_pages {
+	struct list_head list;
+	struct page *page;
+	block_t old_addr;		/* for revoking when fail to commit */
+	nid_t nid;
+};
+
 struct sit_info {
 	const struct segment_allocation *s_ops;
 
diff -urN original/fs/f2fs/super.c linux/fs/f2fs/super.c
--- original/fs/f2fs/super.c	2020-08-18 21:43:20.175705250 +0900
+++ linux/fs/f2fs/super.c	2020-09-25 02:50:01.030776672 +0900
@@ -1055,6 +1055,14 @@
 	destroy_percpu_info(sbi);
 	for (i = 0; i < NR_PAGE_TYPE; i++)
 		kfree(sbi->write_io[i]);
+
+	/*
+	 * exF2FS: Hash table for atomic file set
+	 * should be destroyed here
+	 * - Joontaek Oh.
+	 */
+	rhashtable_destroy(&sbi->afs_ht);
+
 	kfree(sbi);
 }
 
@@ -2663,6 +2671,7 @@
 	char *options = NULL;
 	int recovery, i, valid_super_block;
 	struct curseg_info *seg_i;
+	//printk("[JATA DBG] (%s) sb->s_dev: %u\n", __func__, sb->s_dev);
 
 try_onemore:
 	err = -EINVAL;
@@ -3013,8 +3022,28 @@
 				cur_cp_version(F2FS_CKPT(sbi)));
 	f2fs_update_time(sbi, CP_TIME);
 	f2fs_update_time(sbi, REQ_TIME);
+
+	/*
+	 * exF2FS: Initialize hash table for atomic file set.
+	 * If it fails, it jumps to *free_afs_hash* section,
+	 * Deallocate everything.
+	 * - Joontaek Oh.
+	 */
+	sbi->afs_kht_params.head_offset = offsetof(struct atomic_file_set, khtnode),
+	sbi->afs_kht_params.key_offset = offsetof(struct atomic_file_set, key),
+	sbi->afs_kht_params.key_len = FIELD_SIZEOF(struct atomic_file_set, key),
+	sbi->afs_kht_params.locks_mul = 1,
+	sbi->afs_kht_params.automatic_shrinking = true,
+
+	err = rhashtable_init(&sbi->afs_ht, &sbi->afs_kht_params);
+	if (err)
+		goto free_afs_hash;
+
 	return 0;
 
+free_afs_hash:
+	rhashtable_destroy(&sbi->afs_ht);
+
 free_meta:
 #ifdef CONFIG_QUOTA
 	if (f2fs_sb_has_quota_ino(sb) && !f2fs_readonly(sb))
